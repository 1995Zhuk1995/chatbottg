# Вопросно-ответная диалоговая система

## Консольный фронтенд для бота

Реализован в файле [test_simple_console_answering_machine.py](https://github.com/Koziev/chatbot/blob/master/PyModels/bot/test_simple_console_answering_machine.py)

![Console frontend for chatbot](chatbot-console.PNG)

## Чатбот для Telegram

Реализован в файле [test_telegram_bot.py](https://github.com/Koziev/chatbot/blob/master/PyModels/bot/test_telegram_bot.py)

![Telegram frontend for chatbot](chatbot-telegram.png)


## Тренировка и использование модели посимвольного встраивания слов

Модели чат-бота при работе используют разные способы векторного представления слов.
Один из использованных алгоритмов векторизации заключается в упаковке посимвольного
представления слова с помощью сжимающего рекуррентного автоэнкодера:

![wordchar2vector model architecture](wordchar2vector.arch.png)

На вход подается цепочка символов слова. Эта цепочка упаковывается в вектор фиксированной
длины с помощью LSTM-слоя. Вектор, получаемый на выходе этого слоя содержит всю информацию
о символах, 2-граммах и т.д. в исходном слове. Это позволяет прочим моделям учитывать как семантическую, там и морфологическую близость слов. К примеру,
слова "использован" и "использовал" в большинстве языковых контекстов образуют близкие
по смыслу предложения благодаря общему корню. В других ситуация важную информацию несут
суффиксы, окончания или приставки. Посимвольные встраивания слов доставляют эту информацию.

Выходная часть автоэнкодера, как обычно, восстанавливает исходную цепочку символов слова из
вектора.

### Основной настроечный параметр модели

Качество и сложность модели и использующих ее алгоритмов регулируется параметром "размер вектора слова",
который задается опцией --dims NN в командой строке. По умолчанию этот параметр равен 56.

### Тренировка модели wordchar2vector

Для тренировки посимвольных встраиваний необходимо выполнить два шага.

Во-первых, сгенерировать список слов. Эту задачу выполняет скрипт [prepare_wordchar_dataset.py](https://github.com/Koziev/chatbot/blob/master/PyModels/prepare_wordchar_dataset.py).
Он читает несколько других датасетов и текстовых файлов и сохраняет итоговый список
слов в файле [../tmp/known_words.txt](https://github.com/Koziev/chatbot/blob/master/tmp/known_words.txt).
Вариант этого файла выложен в репозиторий, поэтому первый шаг можно пропустить.

Во-вторых, запустить тренировку модели скриптом [wordchar2vector.py](https://github.com/Koziev/chatbot/blob/master/PyModels/wordchar2vector.py)
примерно такой командой:

```
python ../PyModels/wordchar2vector.py --train 1 --i ../tmp/known_words.txt --o ../tmp/wordchar2vector.dat --model_dir ../tmp --dims 56
```

Для запуска обучения можно взять готовый скрипт ../scripts/train_wordchar2vector.cmd или
../scripts/train_wordchar2vector.sh, в которых указаны пути к входным и выходным файлам.

Тренировка модели сопровождается наглядной визуализацией текущего состояния модели:

![wordchar2vector training](wordchar2vector.PNG)

### Векторизация лексикона без перетренировки модели

Тренировочный скрипт сохраняет на диске саму модель (файлы с метаданными для восстановления
нейросетки и веса). Поэтому новые слова в поступающих от собеседника фразах можно
векторизовать на лету, если в готовом наборе векторов их еще нет. 

Если задать в командной строке опции --train 0 --vectorize 1, то будет загружены файлы
ранее обученной модели из каталога, указанного опцией --model_dir, и слова из входного
файла будут векторизованы и сохранены в выходном файле.

```
python ../PyModels/wordchar2vector.py --train 0 --vectorize 1 --i ../tmp/known_words.txt --o ../tmp/wordchar2vector.dat --model_dir ../tmp --dims 56
```

Этот режим позволяет быстро векторизовать лексикон после добавления новых
предложений в исходные датасеты, не переучивая нейросеть.

### Где используется модель wordchar2vector

Загрузка и использование векторых моделей слов в чатботе инкапсулировано в классе [WordEmbeddings](https://github.com/Koziev/chatbot/blob/master/PyModels/bot/word_embeddings.py)
